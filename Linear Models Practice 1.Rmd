---
title: "R Notebook"
output: html_notebook
---
This exercise covers sales and promotion data modelling, from the basic linear regression to mixed effects model. We will go over them one by one. Please note that here the data is hypothetical, and for academic use only. Due to copyright restriction I cannot share the datasets with anyone outside McCombs Business School. 

Loading in sales data of a local store 50 weeks before and 50 weeks after the opening of a Walmart in the area. 
```{r}
wm.dat = read.csv("Walmart_Data.csv", header = T)
str(wm.dat)
```
```{r}
summary(wm.dat)
```
We will make a new variable called "logSales", for a better linear fit.
```{r}
wm.dat$logSales = log(wm.dat$Sales)
```
To compute our first linear model, it is important to understand the correlation of each variable here - this is to include any interactions in our model later. The variables of "Sales", "Promotion", and "Feature" are stored in the 2,3,4th columns. 
```{r}
cor(wm.dat[,2:4])
```
Scatterplot of Sales and Promotion, and Sales and Feature:
```{r}
plot(wm.dat$Sales,wm.dat$Promotion)
```
Similarly, for Sales and Feature
```{r}
plot(wm.dat$Sales,wm.dat$Feature)
```
We notice the heterogenecity here in both plots. We will consider these interactions later. Now a histogram of Sales for distribution:
```{r}
histogram(wm.dat$Sales)
histogram(wm.dat$logSales)
```
Now we will use the simplest linear model to test out:
```{r}
wm.lm1 = lm(logSales~Promotion+Feature+Walmart+Holiday, data=wm.dat)
summary(wm.lm1)
```
The estimated coefficients correspond to lift/effect each parameter has on the log of Sales of the store (a percentage lift). 
The coefficient of Promotion is 0.84754, meaning when promotion is up by 1%, Sales goes up by EXP(0.84754) %. 
```{r}
round(exp(wm.lm1$coefficients),4)
```
Similarly, when Feature goes up by 1%, Sales goes up by EXP(0.75076) %. When Warmart is in town, Sales decrease by EXP(-0.31127) %. Finally, when HolidayYes is “switched on”, meaning it is a holiday, Sales goes up by EXP(0.26004) %. All numbers are shown above.

Now we include the interactions between parameters to account for the covariance we observed earlier:
```{r}
wm.lm2 = lm(logSales~Promotion+Feature+Walmart+Holiday+Holiday:Walmart+Holiday:Promotion, data = wm.dat)
summary(wm.lm2)
```
```{r}
round(exp(wm.lm2$coefficients),4)
```
```{r}
AIC(wm.lm1) 
AIC(wm.lm2)
BIC(wm.lm1)
BIC(wm.lm2)
```
Model comparison verdict: first model wins. Second model has too many parameters (penalized here by BIC), and some are not even significant!

Another popular model is using random effects and hierarchical linear models. They are best suited when we have repeated obversations of the same ID, e.g. customer's all recent transactions. We will use a credit card company data. 

The credit card company would like to figure out whether offering more promotions (for example, gasoline rebates and coupons for using the credit card) to their existing customers can increase the share-of-wallet of the credit card (that is, the share of a consumer's monthly spending using the credit card in her total spending). The company would also like to figure out what customer characteristics make them more responsive to promotions. 
The company conducted a field experiment by randomly selecting 300 customers and offering them different monthly promotions for 12 months. The share-of-wallet data were recorded in each month for every customer. The data set also included some consumer characteristics. 

```{r}
sow = read.csv("CreditCard_SOW_data.csv", header = T)
summary(sow)
str(sow)
```
ConsumerID here should not be int., but rather a key, an ID that indicates a level. 
```{r}
sow$ConsumerID = as.factor(sow$ConsumerID)
```
Now we add in logIncome and logSowRatio for calculus purpose.
```{r}
sow$logIncome = log(sow$Income)
sow$logSowRatio = log(sow$WalletShare/(1-sow$WalletShare))
str(sow)
```
Build our first linear model:
```{r}
sow.lm1 = lm(logSowRatio~History+Balance+Promotion+History:Promotion+logIncome:Promotion, data = sow)
summary(sow.lm1)
```
The problem here is that this model does NOT show how each customer behaves on their individual level. We want a hierarchy to represent both the overall aggregated model AND each customer ID's estimation. i, j here are indices. E represents unexplained residues. 

logSowRatioij = β0i + β1×Balanceij + β2i×Promotionij + βij

β0i = β0 +β1×Historyi +Ei   

β2i = β0 +β1×Historyi +β2×logIncomei +Ei   

Rearrange the equation and we get our one-level linear regression model with random effects:

Here the parameters that are not indexed with i are fixed effects: History, Balance, History:Promotion, logIncome:Promotion
… as opposed to those indexed with I are random effects: ConsumerID, ConsumerID:Promotion 


We use a package called 'lme4'.
```{r}
library(lme4)
sow.re1 = lmer(logSowRatio~History+Balance+Promotion+History:Promotion+logIncome:Promotion+(1+Promotion|ConsumerID), data = sow, REML = F, control = lmerControl(optimizer="Nelder_Mead"))
summary(sow.re1)

```
The interpretation of the coefficients here would be:
History, Promotion have a positive effect on the sow score, while Balance, History:Promotion, and Promotion:logIncome have a negative effect on the sow score. This means, the longer history the customer has with the company, the more promotion he/she receives, the more likely he/she will use the credit card. The more balance he/she has on the card, the less likely he/she will use the card - this makes sence, since credit card balance is a debt you owe the company. However, promotion might tire a customer out if it's repeated too many time, hence History:Promotion has a negative effect on the sow score. Promotion also seems to have a negative effect when it's couple with logIncome. This might be interpreted as higher income customers perceive promotions negatively wherreas lower income customers are more promotion-sensitive. 

Now let's look at AIC/BIC to see which model explains better.
```{r}
AIC(sow.lm1)
AIC(sow.re1)
BIC(sow.lm1)
BIC(sow.re1)
```
Linear mixed effect is a much better model here!
